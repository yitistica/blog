
<!DOCTYPE html>


<html lang="en" data-content_root="../../" data-theme="dark">

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Setting Up an LLM Server on Your Home Kubernetes Cluster &#8212; qiuyids.me</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "dark";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "dark";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=87e54e7c" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css?v=f1baa633" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=a5603611"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=6b89569b"></script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'posts/set_up_ollama_with_gpu_on_kubernetes';</script>
    <link rel="icon" href="../../_static/y_logo_icononly.png"/>
    <link rel="author" title="About these documents" href="../../about/" />
    <link rel="index" title="Index" href="../../genindex/" />
    <link rel="search" title="Search" href="../../search/" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>



  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="dark">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search/"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../">
  
  
  
  
  
    
    
    
    <img src="../../_static/yitistica_logo.png" class="logo__image only-dark" alt="qiuyids.me - Home"/>
    <script>document.write(`<img src="../../_static/yitistica_logo.png" class="logo__image only-light" alt="qiuyids.me - Home"/>`);</script>
  
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../custom-post-list/">
    Posts
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../about/">
    About
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
        </div>
      
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/yitistica" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-square-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://www.linkedin.com/in/qiuyids" title="Linkedin" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-linkedin fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Linkedin</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../custom-post-list/">
    Posts
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../about/">
    About
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/yitistica" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-square-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://www.linkedin.com/in/qiuyids" title="Linkedin" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-linkedin fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Linkedin</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
    <div class="sidebar-start-items__item">
    
    
    <h5>
        
        <h4 class="meta_section_header">About</h4>
        <ul class="post_meta_ul">
            
            <li>
                <span>
                
                <i class="fa fa-calendar"></i>
                
                2024-07-26
                </span>
            </li>
            
            <li>
                <span>
                
                </span>
            </li>
        </ul>
        
    </h5>
</div>

    

<h4 class="meta_section_header"><a href="../../blog/tag/">With Tags</a></h4>
<ul class="tags_ul">
    
    
    <li class="tag_li">
        <a href="../../blog/tag/kubernetes/"><span class="sd-badge">Kubernetes</span></a></li>
    
    
    
    <li class="tag_li">
        <a href="../../blog/tag/homelab/"><span class="sd-badge">homelab</span></a></li>
    
    
    
    <li class="tag_li">
        <a href="../../blog/tag/set-up-guide/"><span class="sd-badge">set-up-guide</span></a></li>
    
    
    
    <li class="tag_li">
        <a href="../../blog/tag/ubuntu-server/"><span class="sd-badge">Ubuntu Server</span></a></li>
    
    
    
    <li class="tag_li">
        <a href="../../blog/tag/llm/"><span class="sd-badge">LLM</span></a></li>
    
    
    
    <li class="tag_li">
        <a href="../../blog/tag/ollama/"><span class="sd-badge">Ollama</span></a></li>
    
    
    
    <li class="tag_li">
        <a href="../../blog/tag/gpu/"><span class="sd-badge">GPU</span></a></li>
    
    
    
    <li class="tag_li">
        <a href="../../blog/tag/nvidia/"><span class="sd-badge">NVIDIA</span></a></li>
    
    
</ul>

    


<div>
<h4 class="meta_section_header">
  <a href="../../blog/category/">On the Same Topic</a>
</h4>
<ul class="category_ul">
  
  
  
  
  <li class="category_li">
    <a href="../../blog/category/devops/"><span>devops (3)</span></a>
  </li>
  
  
</ul>
</div>

    <div>
    
    
    <h4 class="meta_section_header">
        <a href="../../blog/">Recently Written</a>
    </h4>
    <ul class="category_ul">
        
        <li class="category_li">
            <a href="../what_is_agentic_system/">
                <span>My Two Cents on Agentic AI</span>
            </a>
            <span> - 2024-08-25</span>
        </li>
        
        <li class="category_li">
            <a href="../set_up_kubernetes_cluster/">
                <span>Deploying a Local Kubernetes Cluster on Ubuntu Servers</span>
            </a>
            <span> - 2024-07-15</span>
        </li>
        
        <li class="category_li">
            <a href="../set_up_docker_image_registry_for_kubernetes/">
                <span>Setting Up a Private Docker Registry</span>
            </a>
            <span> - 2024-07-12</span>
        </li>
        
    </ul>
    
</div>
</div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page">Setting Up...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
<section id="setting-up-an-llm-server-on-your-home-kubernetes-cluster">
<h1>Setting Up an LLM Server on Your Home Kubernetes Cluster<a class="headerlink" href="#setting-up-an-llm-server-on-your-home-kubernetes-cluster" title="Link to this heading">#</a></h1>
<p>The new Llama 3.1 is <a class="reference external" href="https://ai.meta.com/blog/meta-llama-3-1/">out</a>, and the fact that a smaller quantized version of the model can be easily deployed in a home environment is quite exciting. If you have some spare computing resources such as a GPU, I will totally recommend deploying an LLM server on your PC or home server for good reasons. On the one hand, you no longer have to pay a subscription fee for a limited-usage service if you primarily use it for less complex tasks. On the other hand, smaller models offer the advantages of better energy efficiency, privacy, stable availability, and the potential of custom integration with other local services like smart home assistants.</p>
<p>In this blog post, I will go through setting up an LLM server on my home Kubernetes cluster. I will be using <a class="reference external" href="https://hub.docker.com/r/ollama/ollama">Ollama in a container</a> to host the LLM service, and optionally, deploy a client web interface with Open WebUI. While the UI is not a requirement to my own needs, it is nice to have since you can easily share the service with others in the same network. Therefore I will be deploying an Open WebUI instance alongside as well. My Kubernetes cluster with version 1.30 is running on multiple Ubuntu 24.04 Server nodes.</p>
<p>A quick reminder: for those planning to use LLMs on a PC or seeking a simpler installation on a server with Docker, there are more straightforward options available that don’t require a more complex Kubernetes configuration. Ollama does provide native <a class="reference external" href="https://ollama.com/download/windows">installers</a> for macOS, Linux, and Windows. Alternatively, spawning a Docker container directly is another option for local deployment. It’s also worth noting that this guide demonstrates the use of CUDA capable GPUs from NVIDIA. Setting up GPUs from other vendors may require different steps, which won’t be covered here.</p>
<p>For the audience of this post, I will assume you already have a Kubernetes cluster, if not, you may check out my other <a class="reference internal" href="../set_up_kubernetes_cluster/"><span class="doc">post</span></a> for setting one up.</p>
<p>In this guide, we’ll cover the following tasks to set up an LLM server on your home Kubernetes cluster:</p>
<ol class="arabic simple">
<li><p>Configuring a worker node for GPU support</p></li>
<li><p>configuring your Kubernetes cluster to use the GPUs</p></li>
<li><p>deploying Ollama on the cluster</p></li>
<li><p>optionally, deploying Open WebUI</p></li>
</ol>
<section id="setting-up-a-gpu-supported-node-nvidia">
<h2>Setting Up a GPU Supported Node (NVIDIA)<a class="headerlink" href="#setting-up-a-gpu-supported-node-nvidia" title="Link to this heading">#</a></h2>
<p>The first step is to ensure that your worker node has GPU properly configured. While Ollama’s LLM serving part doesn’t strictly require a GPU to run its model hosting service, having one is crucial for efficient LLM inference. Without a GPU, processing times can become painfully long. In this guide, we’ll focus on setting up NVIDIA GPUs, as they’re the most common choice for deep learning tasks.</p>
<p><strong>0. What do we need to install from NVIDIA?</strong></p>
<p>Please check if you have a <a class="reference external" href="https://developer.nvidia.com/cuda-gpus">CUDA capable GPU</a>; and check if the drivers supports your <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#system-requirements">operating system</a>. For our purpose, we’ll follow the <a class="reference external" href="https://docs.nvidia.com/datacenter/tesla/drivers/index.html#deployment-workflow">recommended approach</a> of installing only the NVIDIA drivers.</p>
<p><strong>1. Check If the GPU is Recognized by Your Host Node</strong></p>
<p>You need to ensure that the GPU is recognized by your host node, This step confirms that your hardware setup is correct.</p>
<div class="literal-block-wrapper docutils container" id="id2">
<div class="code-block-caption"><span class="caption-text">Command: check if the GPU is recognized</span><a class="headerlink" href="#id2" title="Link to this code">#</a></div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>lspci<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>-i<span class="w"> </span>nvidia
</pre></div>
</div>
</div>
<p>If your GPU is properly connected, you should see at least one line of output displaying your GPU’s name and version, even if you have not installed a driver. If you don’t see any output, then you may need to check your hardware connection or GPU passthrough settings if you’re using a virtual machine.</p>
<p><strong>2*. Clean Out all NVIDIA Related Packages</strong></p>
<p>To avoid potential package conflicts, it’s a good idea to remove any existing NVIDIA packages before installing the driver. This step assumes your GPU will be dedicated to the Kubernetes cluster only.</p>
<div class="literal-block-wrapper docutils container" id="id3">
<div class="code-block-caption"><span class="caption-text">Command: removing CUDA toolkit and drivers (Ubuntu and Debian only)</span><a class="headerlink" href="#id3" title="Link to this code">#</a></div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo<span class="w"> </span>apt-get<span class="w"> </span>--purge<span class="w"> </span>remove<span class="w"> </span><span class="s2">&quot;*cuda*&quot;</span><span class="w"> </span><span class="s2">&quot;*cublas*&quot;</span><span class="w"> </span><span class="s2">&quot;*cufft*&quot;</span><span class="w"> </span><span class="s2">&quot;*cufile*&quot;</span><span class="w"> </span><span class="s2">&quot;*curand*&quot;</span><span class="w"> </span><span class="s2">&quot;*cusolver*&quot;</span><span class="w"> </span><span class="s2">&quot;*cusparse*&quot;</span><span class="w"> </span><span class="s2">&quot;*gds-tools*&quot;</span><span class="w"> </span><span class="s2">&quot;*npp*&quot;</span><span class="w"> </span><span class="s2">&quot;*nvjpeg*&quot;</span><span class="w"> </span><span class="s2">&quot;nsight*&quot;</span><span class="w"> </span><span class="s2">&quot;*nvvm*&quot;</span>

sudo<span class="w"> </span>apt-get<span class="w"> </span>--purge<span class="w"> </span>remove<span class="w"> </span><span class="s2">&quot;*nvidia*&quot;</span><span class="w"> </span><span class="s2">&quot;libxnvctrl*&quot;</span>

sudo<span class="w"> </span>apt-get<span class="w"> </span>autoremove
</pre></div>
</div>
</div>
<p><strong>3 Install the NVIDIA Drivers (Canonical’s Installation and Using the APT Package Manger)</strong></p>
<p>The recommended way of installing NVIDIA drivers on a Ubuntu system is to use the <a class="reference external" href="https://ubuntu.com/server/docs/nvidia-drivers-installation">Ubuntu-Drivers Tool</a> especially when your system uses Secure Boot.</p>
<div class="literal-block-wrapper docutils container" id="id4">
<div class="code-block-caption"><span class="caption-text">Command: install the driver through Ubuntu-Drivers Tool</span><a class="headerlink" href="#id4" title="Link to this code">#</a></div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo<span class="w"> </span>ubuntu-drivers<span class="w"> </span>list<span class="w"> </span>--gpgpu

sudo<span class="w"> </span>ubuntu-drivers<span class="w"> </span>install<span class="w"> </span>--gpgpu<span class="w"> </span>nvidia:535-server
</pre></div>
</div>
</div>
<p><code class="code highlight bash docutils literal highlight-bash">nvidia:535-server</code> is the version of the driver you want to install, <code class="code highlight bash docutils literal highlight-bash"><span class="m">535</span>-server</code> means the server version of the 535 branch. Replace this with the version of the driver you want to install. The driver version can be found by running the first command.</p>
<p>It should work out most of the time. Unfortunately for me, I might have been caught in temporary compatibility issues. I had to install the driver differently using the APT Package Manger. You may use the <a class="reference external" href="https://developer.nvidia.com/cuda-downloads">download builder</a> to find the correct driver version for your system.</p>
<div class="literal-block-wrapper docutils container" id="id5">
<div class="code-block-caption"><span class="caption-text">Command: install the drivers using APT</span><a class="headerlink" href="#id5" title="Link to this code">#</a></div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>wget<span class="w"> </span>https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-keyring_1.1-1_all.deb
sudo<span class="w"> </span>dpkg<span class="w"> </span>-i<span class="w"> </span>cuda-keyring_1.1-1_all.deb
sudo<span class="w"> </span>apt-get<span class="w"> </span>update
sudo<span class="w"> </span>apt-get<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>nvidia-driver-535-server
sudo<span class="w"> </span>apt-get<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>cuda-drivers-535
sudo<span class="w"> </span>reboot
</pre></div>
</div>
</div>
<p><strong>4 Final Check for Driver Status</strong></p>
<p>After installation and reboot, verify that the driver is properly installed.</p>
<div class="literal-block-wrapper docutils container" id="id6">
<div class="code-block-caption"><span class="caption-text">Command: check if the driver is installed properly</span><a class="headerlink" href="#id6" title="Link to this code">#</a></div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>dpkg<span class="w"> </span>-l<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>nvidia-driver

nvidia-smi
</pre></div>
</div>
</div>
<p><code class="code highlight bash docutils literal highlight-bash">dpkg<span class="w"> </span>-l<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>nvidia-driver</code> checks if there is a driver installed. <code class="code highlight bash docutils literal highlight-bash">nvidia-smi</code> command should produce output similar to this.</p>
<div class="no-copybutton console-output highlight-console notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="go">+---------------------------------------------------------------------------------------+</span>
<span class="linenos"> 2</span><span class="go">| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |</span>
<span class="linenos"> 3</span><span class="go">|-----------------------------------------+----------------------+----------------------+</span>
<span class="linenos"> 4</span><span class="go">| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |</span>
<span class="linenos"> 5</span><span class="go">| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |</span>
<span class="linenos"> 6</span><span class="go">|                                         |                      |               MIG M. |</span>
<span class="linenos"> 7</span><span class="go">|=========================================+======================+======================|</span>
<span class="linenos"> 8</span><span class="go">|   0  NVIDIA GeForce RTX 3060        Off | 00000000:01:00.0 Off |                  N/A |</span>
<span class="linenos"> 9</span><span class="go">|  0%   44C    P8              17W / 170W |      3MiB / 12288MiB |      0%      Default |</span>
<span class="linenos">10</span><span class="go">|                                         |                      |                  N/A |</span>
<span class="linenos">11</span><span class="go">+-----------------------------------------+----------------------+----------------------+</span>
<span class="linenos">12</span>
<span class="linenos">13</span><span class="go">+---------------------------------------------------------------------------------------+</span>
<span class="linenos">14</span><span class="go">| Processes:                                                                            |</span>
<span class="linenos">15</span><span class="go">|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |</span>
<span class="linenos">16</span><span class="go">|        ID   ID                                                             Usage      |</span>
<span class="linenos">17</span><span class="go">|=======================================================================================|</span>
<span class="linenos">18</span><span class="go">|  No running processes found                                                           |</span>
<span class="linenos">19</span><span class="go">+---------------------------------------------------------------------------------------+</span>
</pre></div>
</div>
<p>This output confirms that your NVIDIA driver is installed and functioning correctly.</p>
</section>
<section id="installing-nvidia-container-toolkit">
<h2>Installing NVIDIA Container Toolkit<a class="headerlink" href="#installing-nvidia-container-toolkit" title="Link to this heading">#</a></h2>
<p>Having the driver installed could only allow us to use GPUs natively on the host system. We still need to install <strong>NVIDIA Container Toolkit</strong>, which enables users to run GPU-enabled containers. We can install it using the following steps.</p>
<div class="literal-block-wrapper docutils container" id="id7">
<div class="code-block-caption"><span class="caption-text">Command: Add NVIDIA repository</span><a class="headerlink" href="#id7" title="Link to this code">#</a></div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>curl<span class="w"> </span>-fsSL<span class="w"> </span>https://nvidia.github.io/libnvidia-container/gpgkey<span class="w"> </span><span class="p">|</span><span class="w"> </span>sudo<span class="w"> </span>gpg<span class="w"> </span>--dearmor<span class="w"> </span>-o<span class="w"> </span>/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg<span class="w"> </span><span class="se">\</span>
<span class="w">  </span><span class="o">&amp;&amp;</span><span class="w"> </span>curl<span class="w"> </span>-s<span class="w"> </span>-L<span class="w"> </span>https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list<span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>sed<span class="w"> </span><span class="s1">&#39;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g&#39;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>sudo<span class="w"> </span>tee<span class="w"> </span>/etc/apt/sources.list.d/nvidia-container-toolkit.list
</pre></div>
</div>
</div>
<p>We first add the NVIDIA repository to your system and sets up the GPG key for package verification.</p>
<div class="literal-block-wrapper docutils container" id="id8">
<div class="code-block-caption"><span class="caption-text">Command: install NVIDIA Container Toolkit</span><a class="headerlink" href="#id8" title="Link to this code">#</a></div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo<span class="w"> </span>apt-get<span class="w"> </span>update
sudo<span class="w"> </span>apt-get<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>nvidia-container-toolkit
</pre></div>
</div>
</div>
<p>We then update package index and install the NVIDIA Container Toolkit.</p>
<div class="literal-block-wrapper docutils container" id="id9">
<div class="code-block-caption"><span class="caption-text">Command: configure runtime</span><a class="headerlink" href="#id9" title="Link to this code">#</a></div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo<span class="w"> </span>nvidia-ctk<span class="w"> </span>runtime<span class="w"> </span>configure<span class="w"> </span>--runtime<span class="o">=</span>containerd

sudo<span class="w"> </span>nano<span class="w"> </span>/etc/containerd/config.toml
</pre></div>
</div>
</div>
<div class="literal-block-wrapper docutils container" id="id10">
<div class="code-block-caption"><span class="caption-text">/etc/containerd/config.toml</span><a class="headerlink" href="#id10" title="Link to this code">#</a></div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nn">...</span>
<span class="p p-Indicator">[</span><span class="nv">plugins</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="p p-Indicator">[</span><span class="nv">plugins.&quot;io.containerd.grpc.v1.cri&quot;</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">[</span><span class="nv">plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd</span><span class="p p-Indicator">]</span>
<span class="hll"><span class="w">      </span><span class="l l-Scalar l-Scalar-Plain">default_runtime_name = &quot;nvidia&quot;</span>
</span></pre></div>
</div>
</div>
<p>We configure the runtime to use the NVIDIA Runtime by default by editing the containerd configuration file to set the default runtime name to ‘nvidia’.</p>
<div class="literal-block-wrapper docutils container" id="id11">
<div class="code-block-caption"><span class="caption-text">Command: restart containerd</span><a class="headerlink" href="#id11" title="Link to this code">#</a></div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo<span class="w"> </span>systemctl<span class="w"> </span>restart<span class="w"> </span>containerd
</pre></div>
</div>
</div>
<p>We have to restart the containerd service to apply the changes.</p>
</section>
<section id="installing-nvidia-gpu-operator-in-kubernetes">
<h2>Installing NVIDIA GPU Operator in Kubernetes<a class="headerlink" href="#installing-nvidia-gpu-operator-in-kubernetes" title="Link to this heading">#</a></h2>
<p>While many would choose to install <strong>NVIDIA device plugin</strong> for basic GPU usage in the Kubernetes environment, <strong>NVIDIA GPU Operator</strong> provides a more comprehensive solution to use GPUs in Kubernetes. <strong>NVIDIA GPU Operator</strong> is a plugin that you deploy to your Kubernetes cluster to automate the management of all NVIDIA software components needed to provision GPU. In addition, it allows advanced configuration such as time-slicing, which enables workloads that are scheduled on oversubscribed GPUs to interleave with one another.</p>
<p>In this blog post, we’ll go through the process of installing the <strong>NVIDIA GPU Operator</strong> in a Kubernetes cluster.</p>
<p><strong>1*. Installing Helm</strong></p>
<p>Helm is a package manager for Kubernetes that simplifies the deployment and management of applications by using charts to pre-configure Kubernetes resources. As NVIDIA’s official guide uses Helm to install the GPU Operator, we will follow and use Helm as well.</p>
<p>If you haven’t installed Helm yet, this is snipped from the official <a class="reference external" href="https://helm.sh/docs/intro/install/">Helm installation documentation</a>:</p>
<div class="literal-block-wrapper docutils container" id="id12">
<div class="code-block-caption"><span class="caption-text">Command: helm installation (on the controller node)</span><a class="headerlink" href="#id12" title="Link to this code">#</a></div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>curl<span class="w"> </span>https://baltocdn.com/helm/signing.asc<span class="w"> </span><span class="p">|</span><span class="w"> </span>gpg<span class="w"> </span>--dearmor<span class="w"> </span><span class="p">|</span><span class="w"> </span>sudo<span class="w"> </span>tee<span class="w"> </span>/usr/share/keyrings/helm.gpg<span class="w"> </span>&gt;<span class="w"> </span>/dev/null
sudo<span class="w"> </span>apt-get<span class="w"> </span>install<span class="w"> </span>apt-transport-https<span class="w"> </span>--yes
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;deb [arch=</span><span class="k">$(</span>dpkg<span class="w"> </span>--print-architecture<span class="k">)</span><span class="s2"> signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main&quot;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>sudo<span class="w"> </span>tee<span class="w"> </span>/etc/apt/sources.list.d/helm-stable-debian.list
sudo<span class="w"> </span>apt-get<span class="w"> </span>update
sudo<span class="w"> </span>apt-get<span class="w"> </span>install<span class="w"> </span>helm
</pre></div>
</div>
</div>
<p>This series of commands adds the Helm repository, installs necessary dependencies, and finally installs Helm itself.</p>
<p><strong>2. Installing NVIDIA GPU Operator</strong></p>
<p>Now that we have Helm installed, we can use it to install <strong>NVIDIA GPU Operator</strong>.</p>
<div class="literal-block-wrapper docutils container" id="id13">
<div class="code-block-caption"><span class="caption-text">Command: install GPU operator</span><a class="headerlink" href="#id13" title="Link to this code">#</a></div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># install GPU operator:</span>
helm<span class="w"> </span>install<span class="w"> </span>--wait<span class="w"> </span>--generate-name<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span>--create-namespace<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>nvidia/gpu-operator<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--set<span class="w"> </span>driver.enabled<span class="o">=</span><span class="nb">false</span><span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--set<span class="w"> </span>toolkit.enabled<span class="o">=</span><span class="nb">false</span><span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--timeout<span class="w"> </span>15m
</pre></div>
</div>
</div>
<p>As we have pre-installed the <strong>NVIDIA GPU driver</strong> and <strong>NVIDIA Container toolkit</strong> manually before, we should disable the installation of the driver and toolkit in the GPU Operator setup by setting <code class="code highlight bash docutils literal highlight-bash">driver.enabled</code> and <code class="code highlight bash docutils literal highlight-bash">toolkit.enabled</code> to <code class="code highlight bash docutils literal highlight-bash"><span class="nb">false</span></code>.</p>
<p>If you have not installed both of them, you may set them to true, which automatically installs the driver and toolkit and configures the runtime for you. The reason why I still went through those steps earlier here is that no matter what method you use, you will need to install the driver and toolkit explicitly or implicitly. For those who do not use NVIDIA GPU operator, you may install an alternative instead.</p>
<div class="literal-block-wrapper docutils container" id="id14">
<div class="code-block-caption"><span class="caption-text">Command: check the statuses of the GPU operator pods</span><a class="headerlink" href="#id14" title="Link to this code">#</a></div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl<span class="w"> </span>get<span class="w"> </span>all<span class="w"> </span>-n<span class="w"> </span>gpu-operator
</pre></div>
</div>
</div>
<p>You should expect all pods are running or complete as their status.</p>
<p><strong>3. Label the GPU Nodes for Affinity</strong></p>
<p>The final Kubernetes configuration is to label the nodes that have GPUs. This helps in scheduling GPU workloads to the correct nodes. Here’s how you can do it.</p>
<div class="literal-block-wrapper docutils container" id="id15">
<div class="code-block-caption"><span class="caption-text">Command: label nodes with GPU</span><a class="headerlink" href="#id15" title="Link to this code">#</a></div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl<span class="w"> </span>label<span class="w"> </span>nodes<span class="w"> </span>k8s-worker-2<span class="w"> </span><span class="nv">accelerator</span><span class="o">=</span>nvidia-3060-12GB
kubectl<span class="w"> </span>get<span class="w"> </span>nodes<span class="w"> </span>--show-labels
</pre></div>
</div>
</div>
</section>
<section id="running-a-gpu-enabled-container-for-testing">
<h2>Running a GPU Enabled Container (For Testing)<a class="headerlink" href="#running-a-gpu-enabled-container-for-testing" title="Link to this heading">#</a></h2>
<p>Although this section is not directly related to setting up a LLM server, we can quickly abstract the necessary settings for GPU setup in Kubernetes by running a simple container with GPU as a resource. This approach, along with some examinations we can perform, will help us check for GPU utilization and ensure our cluster is properly configured for GPU workloads.</p>
<p><strong>1. Check If Your GPU Is seen in Kubernetes resource</strong></p>
<p>First, we need to verify if Kubernetes can detect your GPU. Run the following command.</p>
<div class="literal-block-wrapper docutils container" id="id16">
<div class="code-block-caption"><span class="caption-text">Command: check if the GPU is recognized by Kubernetes</span><a class="headerlink" href="#id16" title="Link to this code">#</a></div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl<span class="w"> </span>describe<span class="w"> </span>node<span class="w"> </span>k8s-worker-2
<span class="c1"># or</span>
kubectl<span class="w"> </span>describe<span class="w"> </span>node<span class="w"> </span>k8s-worker-2<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>nvidia.com/gpu
</pre></div>
</div>
</div>
<div class="literal-block-wrapper console-output docutils container" id="id17">
<div class="code-block-caption"><span class="caption-text">check if the GPU is recognized by Kubernetes</span><a class="headerlink" href="#id17" title="Link to this code">#</a></div>
<div class="no-copybutton highlight-console notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="go">...</span>
<span class="linenos"> 2</span><span class="go">  Hostname:    k8s-worker-2</span>
<span class="linenos"> 3</span><span class="go">Capacity:</span>
<span class="linenos"> 4</span><span class="go">  cpu:                32</span>
<span class="linenos"> 5</span><span class="go">  ephemeral-storage:  306453440Ki</span>
<span class="linenos"> 6</span><span class="go">  hugepages-1Gi:      0</span>
<span class="linenos"> 7</span><span class="go">  hugepages-2Mi:      0</span>
<span class="linenos"> 8</span><span class="go">  memory:             74096288Ki</span>
<span class="hll"><span class="linenos"> 9</span><span class="go">  nvidia.com/gpu:     1</span>
</span><span class="linenos">10</span><span class="go">  pods:               110</span>
<span class="linenos">11</span><span class="go">...</span>
</pre></div>
</div>
</div>
<p>You should expect to get multiple lines of <code class="code highlight bash docutils literal highlight-bash">nvidia.com/gpu:<span class="w"> </span><span class="m">1</span></code> in the output. You may have multiple GPUs, which will show <code class="code highlight bash docutils literal highlight-bash">nvidia.com/gpu:<span class="w"> </span><span class="m">2</span></code>. or higher. If you don’t see anything, that means your GPU isn’t properly configured or recognized by Kubernetes.</p>
<p><strong>2. Deploy a Simple Container with GPU</strong></p>
<p>Now, let’s deploy a basic container with a GPU that only runs the <code class="code highlight bash docutils literal highlight-bash">nvidia-smi</code> command. This will help us verify if the GPU is properly recognized within the container.</p>
<div class="literal-block-wrapper docutils container" id="id18">
<div class="code-block-caption"><span class="caption-text">GPU-testing-container.yaml</span><a class="headerlink" href="#id18" title="Link to this code">#</a></div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Pod</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gpu-test</span>
<span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">affinity</span><span class="p">:</span>
<span class="w">    </span><span class="nt">nodeAffinity</span><span class="p">:</span>
<span class="w">      </span><span class="nt">requiredDuringSchedulingIgnoredDuringExecution</span><span class="p">:</span>
<span class="w">        </span><span class="nt">nodeSelectorTerms</span><span class="p">:</span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">matchExpressions</span><span class="p">:</span>
<span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">accelerator</span>
<span class="w">            </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">In</span>
<span class="w">            </span><span class="nt">values</span><span class="p">:</span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia-3060-12GB</span>
<span class="w">  </span><span class="nt">containers</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gpu-test</span>
<span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia/cuda:12.1.0-base-ubi8</span>
<span class="w">    </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;nvidia-smi&quot;</span><span class="p p-Indicator">]</span>
<span class="hll"><span class="w">    </span><span class="nt">resources</span><span class="p">:</span>
</span><span class="hll"><span class="w">      </span><span class="nt">limits</span><span class="p">:</span>
</span><span class="hll"><span class="w">        </span><span class="nt">nvidia.com/gpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
</span></pre></div>
</div>
</div>
<p>It is common to encounter incompatibility issue between the host driver and CUDA version in the container, e.g, <code class="code highlight bash docutils literal highlight-bash">requirement<span class="w"> </span>error:<span class="w"> </span>unsatisfied<span class="w"> </span>condition:<span class="w"> </span>cuda&gt;<span class="o">=</span><span class="m">12</span>.5,<span class="w"> </span>please<span class="w"> </span>update<span class="w"> </span>your<span class="w"> </span>driver<span class="w"> </span>to<span class="w"> </span>a<span class="w"> </span>newer<span class="w"> </span>version,<span class="w"> </span>or<span class="w"> </span>use<span class="w"> </span>an<span class="w"> </span>earlier<span class="w"> </span>cuda<span class="w"> </span>container:<span class="w"> </span>unknown</code>. The NVIDIA driver reports a maximum version of CUDA supported (upper right in the table) and can run applications built with CUDA Toolkits up to that version. Ensure you supply the compatible CUDA version in the container image tag or update your driver to a newer version.</p>
<p>If everything goes right, you should expect to see the same table produced by <code class="code highlight bash docutils literal highlight-bash">nvidia-smi</code> in the container log. To check the log run <code class="code highlight bash docutils literal highlight-bash">kubectl<span class="w"> </span>logs<span class="w"> </span>-c<span class="w"> </span>&lt;container-name&gt;<span class="w"> </span>-p<span class="w"> </span>&lt;pod-name&gt;</code>.</p>
<p>This should conclude the basic setup of using GPUs in Kubernetes.</p>
</section>
<section id="gpu-sharing-with-nvidia-gpu-operator">
<h2>GPU Sharing with Nvidia GPU Operator<a class="headerlink" href="#gpu-sharing-with-nvidia-gpu-operator" title="Link to this heading">#</a></h2>
<p>So far your NVIDIA GPUs can only be assigned to a single container instance, which is a significant limitation. This setup means your GPU will be in an idle state most of the time and dedicated to a single service. As we learned earlier, the NVIDIA GPU Operator can be used to enable GPU sharing, allowing multiple workloads to share a single GPU. We will configure the GPU Operator to enable GPU sharing with time slicing.</p>
<p>Setting up GPU sharing is rather simple. All you need to do is add a ConfigMap to the namespace used by the GPU operator. The device plugin will then pick up this ConfigMap and automatically configure GPU time slicing for you.</p>
<p>Here’s an example configuration for GPU time-slicing.</p>
<div class="literal-block-wrapper docutils container" id="id19">
<div class="code-block-caption"><span class="caption-text">time-slicing-config.yaml</span><a class="headerlink" href="#id19" title="Link to this code">#</a></div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ConfigMap</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">time-slicing-config-all</span>
<span class="nt">data</span><span class="p">:</span>
<span class="w">  </span><span class="nt">any</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|-</span>
<span class="w">    </span><span class="no">version: v1</span>
<span class="w">    </span><span class="no">flags:</span>
<span class="w">      </span><span class="no">migStrategy: none</span>
<span class="w">    </span><span class="no">sharing:</span>
<span class="w">      </span><span class="no">timeSlicing:</span>
<span class="w">        </span><span class="no">resources:</span>
<span class="w">        </span><span class="no">- name: nvidia.com/gpu</span>
<span class="w">          </span><span class="no">replicas: 4</span>
</pre></div>
</div>
</div>
<p>By applying this configuration, you’re essentially telling the GPU Operator to divide each GPU into 4 virtual GPUs (i.e., scale resource with name <code class="code highlight bash docutils literal highlight-bash">nvidia.com/gpu</code> to 4 replicas). This allows up to 4 containers to share a single physical GPU. More configurations related to the time slicing feature can be found in the <a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-sharing.html#">official guide</a>.</p>
</section>
<section id="setting-up-ollama-on-kubernetes">
<h2>Setting Up Ollama on Kubernetes<a class="headerlink" href="#setting-up-ollama-on-kubernetes" title="Link to this heading">#</a></h2>
<p><a class="reference external" href="https://github.com/ollama/ollama">Ollama</a> has been a very user-friendly tool that allows you to run LLMs locally. Its simplicity of pulling and running popular models with Ollama makes it accessible for everyone. Although Ollama may have yet to have the robustness and scalability for a large scale production serving, but it is a perfect choice for home use. Here we will go through setting up Ollama on Kubernetes.</p>
<p><strong>1. Create a new Namespace</strong></p>
<p>First, let’s create a dedicated namespace for our Ollama deployment.</p>
<div class="literal-block-wrapper docutils container" id="id20">
<div class="code-block-caption"><span class="caption-text">namespace.yaml</span><a class="headerlink" href="#id20" title="Link to this code">#</a></div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;v1&quot;</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Namespace</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ns-ollama</span>
</pre></div>
</div>
</div>
<p><strong>1. Create Deployment</strong></p>
<p>Now, let’s set up the deployment for Ollama.</p>
<div class="literal-block-wrapper docutils container" id="id21">
<div class="code-block-caption"><span class="caption-text">deployment.yaml</span><a class="headerlink" href="#id21" title="Link to this code">#</a></div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nn">---</span>
<span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">apps/v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deployment</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ollama-deployment</span>
<span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ns-ollama</span>
<span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
<span class="w">  </span><span class="nt">selector</span><span class="p">:</span>
<span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span>
<span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ollama</span>
<span class="w">  </span><span class="nt">template</span><span class="p">:</span>
<span class="w">    </span><span class="nt">metadata</span><span class="p">:</span>
<span class="w">      </span><span class="nt">labels</span><span class="p">:</span>
<span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ollama</span>
<span class="w">    </span><span class="nt">spec</span><span class="p">:</span>
<span class="w">      </span><span class="nt">affinity</span><span class="p">:</span>
<span class="w">        </span><span class="nt">nodeAffinity</span><span class="p">:</span>
<span class="w">          </span><span class="nt">requiredDuringSchedulingIgnoredDuringExecution</span><span class="p">:</span>
<span class="w">            </span><span class="nt">nodeSelectorTerms</span><span class="p">:</span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">matchExpressions</span><span class="p">:</span>
<span class="w">              </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">accelerator</span>
<span class="w">                </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">In</span>
<span class="w">                </span><span class="nt">values</span><span class="p">:</span>
<span class="hll"><span class="w">                </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia-3060-12GB</span>
</span><span class="w">      </span><span class="nt">containers</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ollama</span>
<span class="hll"><span class="w">        </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ollama/ollama:latest</span>
</span><span class="w">        </span><span class="nt">ports</span><span class="p">:</span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">11434</span>
<span class="w">        </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;/bin/sh&quot;</span><span class="p p-Indicator">]</span>
<span class="w">        </span><span class="nt">args</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;-c&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;ollama</span><span class="nv"> </span><span class="s">serve&quot;</span><span class="p p-Indicator">]</span><span class="w">  </span><span class="c1"># Removed --gpus=all flag</span>
<span class="w">        </span><span class="nt">resources</span><span class="p">:</span>
<span class="w">          </span><span class="nt">limits</span><span class="p">:</span>
<span class="hll"><span class="w">            </span><span class="nt">nvidia.com/gpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
</span><span class="w">            </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16</span>
<span class="w">            </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;48Gi&quot;</span>
<span class="w">          </span><span class="nt">requests</span><span class="p">:</span>
<span class="w">            </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8</span>
<span class="w">            </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;24Gi&quot;</span>
<span class="w">        </span><span class="nt">volumeMounts</span><span class="p">:</span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ollama-data</span>
<span class="w">          </span><span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/root/.ollama</span>
<span class="w">      </span><span class="nt">volumes</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ollama-data</span>
<span class="w">        </span><span class="nt">emptyDir</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">{}</span>
<span class="nn">---</span>
<span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Service</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ollama-service</span>
<span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ns-ollama</span>
<span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">selector</span><span class="p">:</span>
<span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ollama</span>
<span class="w">  </span><span class="nt">ports</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">TCP</span>
<span class="w">      </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">11434</span>
<span class="w">      </span><span class="nt">targetPort</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">11434</span>
</pre></div>
</div>
</div>
<p><em>nodeAffinity</em> ensures that the Ollama pods are scheduled on nodes with NVIDIA GeForce RTX 3060 GPUs. Also to allocate GPUs to the pods, you need to specify <strong>nvidia.com/gpu: 1</strong> in the resources section.</p>
<p>It seems the newer <a class="reference external" href="https://github.com/ollama/ollama/releases/tag/v0.2.0">release</a> last month (2024-7) of Ollama supports parallel requests, which means it might not be necessary to run multiple instances for cutting down resource allocation. But we will be running two replicas anyway for this guide since we also want to test out whether GPU time slicing works.</p>
<p>According to the <a class="reference external" href="https://github.com/open-webui/open-webui/discussions/736">discussion</a> and Meta’s <a class="reference external" href="https://llamaimodel.com/requirements/">requirements recommendation</a>, you should expect to allocate at least 8 cores CPU, 16G of RAM and a decent GPU with 8G+ VRAM for a quantized 7B models for a decent experience. I might be running multiple models, so I allocate more RAM limit to each pod.</p>
<p>For simplicity, we will use ephemeral volumes in this guide instead. But I recommend that you should use persistent volume for your data as models can be large in size and there is user data to keep as well. Also shared volume is handy in this case as you do not want to download the same models in every pod.</p>
<div class="literal-block-wrapper console-output docutils container" id="id22">
<div class="code-block-caption"><span class="caption-text">check the pod status of the deployment and if the GPU is recognized in the container</span><a class="headerlink" href="#id22" title="Link to this code">#</a></div>
<div class="no-copybutton highlight-console notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="gp">user@k8s-master-1:~$ </span>kubectl<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>-n<span class="w"> </span>ns-ollama
<span class="linenos"> 2</span><span class="go">NAME                                 READY   STATUS    RESTARTS   AGE</span>
<span class="linenos"> 3</span><span class="go">ollama-deployment-7c76bb65bb-bdpg2   1/1     Running   0          3d15h</span>
<span class="linenos"> 4</span><span class="go">ollama-deployment-7c76bb65bb-lp9wf   1/1     Running   0          3d15h</span>
<span class="linenos"> 5</span><span class="gp">user@k8s-master-1:~$ </span>kubectl<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>-it<span class="w"> </span>ollama-deployment-7c76bb65bb-bdpg2<span class="w"> </span>-n<span class="w"> </span>ns-ollama<span class="w"> </span>--<span class="w"> </span>/bin/bash
<span class="linenos"> 6</span><span class="gp">root@ollama-deployment-7c76bb65bb-bdpg2:/# </span>nvidia-smi
<span class="linenos"> 7</span><span class="go">Thu Aug  1 06:56:21 2024</span>
<span class="linenos"> 8</span><span class="go">+---------------------------------------------------------------------------------------+</span>
<span class="linenos"> 9</span><span class="go">| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |</span>
<span class="linenos">10</span><span class="go">|-----------------------------------------+----------------------+----------------------+</span>
<span class="linenos">11</span><span class="go">| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |</span>
<span class="linenos">12</span><span class="go">| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |</span>
<span class="linenos">13</span><span class="go">|                                         |                      |               MIG M. |</span>
<span class="linenos">14</span><span class="go">|=========================================+======================+======================|</span>
<span class="linenos">15</span><span class="go">|   0  NVIDIA GeForce RTX 3060        Off | 00000000:01:00.0 Off |                  N/A |</span>
<span class="linenos">16</span><span class="go">|  0%   45C    P8              17W / 170W |      3MiB / 12288MiB |      0%      Default |</span>
<span class="linenos">17</span><span class="go">|                                         |                      |                  N/A |</span>
<span class="linenos">18</span><span class="go">+-----------------------------------------+----------------------+----------------------+</span>
<span class="linenos">19</span>
<span class="linenos">20</span><span class="go">+---------------------------------------------------------------------------------------+</span>
<span class="linenos">21</span><span class="go">| Processes:                                                                            |</span>
<span class="linenos">22</span><span class="go">|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |</span>
<span class="linenos">23</span><span class="go">|        ID   ID                                                             Usage      |</span>
<span class="linenos">24</span><span class="go">|=======================================================================================|</span>
<span class="linenos">25</span><span class="go">|  No running processes found                                                           |</span>
<span class="linenos">26</span><span class="go">+---------------------------------------------------------------------------------------+</span>
</pre></div>
</div>
</div>
<p>As both pods seem to be running, we can log into the each container and run <code class="code highlight bash docutils literal highlight-bash">nvidia-smi</code> manually, here we go, one GPU is now allocated to 2 pods.</p>
<p><strong>3. Pulling Ollama Models</strong></p>
<p>Specific models in ollama need to be pulled manually before using them. There are two ways, one is through the command line interface in the pod container, and the other is through the REST API.</p>
<div class="literal-block-wrapper docutils container" id="id23">
<div class="code-block-caption"><span class="caption-text">Command: pulling a model (from within the containers)</span><a class="headerlink" href="#id23" title="Link to this code">#</a></div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ollama<span class="w"> </span>pull<span class="w"> </span>llama3.1
</pre></div>
</div>
</div>
<p>For REST API, you need to ensure you have configured an externally accessible service point, I am using Ingress with NodePort for this guide. Please refer to this section below for <a class="reference internal" href="#id1"><span class="std std-ref">setting up Ingress</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="id24">
<div class="code-block-caption"><span class="caption-text">Command: pulling a model (through the API)</span><a class="headerlink" href="#id24" title="Link to this code">#</a></div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>curl<span class="w"> </span>http://ollama.kube.local:32528/api/pull<span class="w"> </span>-d<span class="w"> </span><span class="s1">&#39;{</span>
<span class="s1">  &quot;name&quot;: &quot;llama3.1&quot;</span>
<span class="s1">}&#39;</span>
</pre></div>
</div>
</div>
</section>
<section id="setting-up-open-webui-for-ollama">
<h2>Setting Up Open WebUI for Ollama<a class="headerlink" href="#setting-up-open-webui-for-ollama" title="Link to this heading">#</a></h2>
<p>We then want to set up a web interface so that general user can interact with the LLM server. <a class="reference external" href="https://github.com/open-webui/open-webui">Open WebUI</a> is a popular choice that supports Ollama.</p>
<p><strong>1. Create a new Namespace</strong></p>
<p>First, we’ll create a dedicated namespace for our Open WebUI deployment.</p>
<div class="literal-block-wrapper docutils container" id="id25">
<div class="code-block-caption"><span class="caption-text">namespace.yaml</span><a class="headerlink" href="#id25" title="Link to this code">#</a></div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Namespace</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">open-webui</span>
</pre></div>
</div>
</div>
<p>Next, we’ll set up the deployment and a service for Open WebUI.</p>
<p><strong>1. Create Deployment</strong></p>
<div class="literal-block-wrapper docutils container" id="id26">
<div class="code-block-caption"><span class="caption-text">deployment.yaml</span><a class="headerlink" href="#id26" title="Link to this code">#</a></div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nn">---</span>
<span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">apps/v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deployment</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">open-webui-deployment</span>
<span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">open-webui</span>
<span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">  </span><span class="nt">selector</span><span class="p">:</span>
<span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span>
<span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">open-webui</span>
<span class="w">  </span><span class="nt">template</span><span class="p">:</span>
<span class="w">    </span><span class="nt">metadata</span><span class="p">:</span>
<span class="w">      </span><span class="nt">labels</span><span class="p">:</span>
<span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">open-webui</span>
<span class="w">    </span><span class="nt">spec</span><span class="p">:</span>
<span class="w">      </span><span class="nt">affinity</span><span class="p">:</span>
<span class="w">        </span><span class="nt">nodeAffinity</span><span class="p">:</span>
<span class="w">          </span><span class="nt">requiredDuringSchedulingIgnoredDuringExecution</span><span class="p">:</span>
<span class="w">            </span><span class="nt">nodeSelectorTerms</span><span class="p">:</span>
<span class="w">              </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">matchExpressions</span><span class="p">:</span>
<span class="w">                  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">kubernetes.io/hostname</span>
<span class="w">                    </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">In</span>
<span class="w">                    </span><span class="nt">values</span><span class="p">:</span>
<span class="hll"><span class="w">                      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">k8s-worker-2</span>
</span><span class="w">      </span><span class="nt">containers</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">open-webui</span>
<span class="hll"><span class="w">        </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">open-webui/open-webui:main</span>
</span><span class="w">        </span><span class="nt">ports</span><span class="p">:</span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8080</span>
<span class="w">        </span><span class="nt">resources</span><span class="p">:</span>
<span class="w">          </span><span class="nt">requests</span><span class="p">:</span>
<span class="w">            </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;500m&quot;</span>
<span class="w">            </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;500Mi&quot;</span>
<span class="w">          </span><span class="nt">limits</span><span class="p">:</span>
<span class="w">            </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;1000m&quot;</span>
<span class="w">            </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;1Gi&quot;</span>
<span class="w">        </span><span class="nt">env</span><span class="p">:</span>
<span class="hll"><span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">OLLAMA_BASE_URL</span>
</span><span class="hll"><span class="w">          </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;http://ollama-service.ns-ollama.svc.cluster.local:11434&quot;</span>
</span><span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">HF_ENDPOINT</span>
<span class="w">          </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">https://hf-mirror.com</span>
<span class="w">        </span><span class="nt">tty</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">        </span><span class="nt">volumeMounts</span><span class="p">:</span>
<span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">open-webui-data</span>
<span class="w">            </span><span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/app/backend/data</span>
<span class="w">      </span><span class="nt">volumes</span><span class="p">:</span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">open-webui-data</span>
<span class="w">          </span><span class="nt">emptyDir</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">{}</span>
<span class="nn">---</span>
<span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Service</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">open-webui-service</span>
<span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">open-webui</span>
<span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">selector</span><span class="p">:</span>
<span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">open-webui</span>
<span class="w">  </span><span class="nt">ports</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">TCP</span>
<span class="w">      </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8080</span>
<span class="w">      </span><span class="nt">targetPort</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8080</span>
</pre></div>
</div>
</div>
<p>I am using the latest official image <em>open-webui/open-webui:main</em>, and specify the resource allocation slightly more than the default. For environment variables, the <strong>OLLAMA_BASE_URL</strong> is set to connect to the Ollama service within the cluster. The <strong>HF_ENDPOINT</strong> is configured to use a mirror for Hugging Face, you may omit this if you don’t have restricted internet. By default, Open WebUI uses port 8080, we simply forward this port to the service.</p>
</section>
<section id="accessing-services-from-outside-the-cluster-ingress-setup">
<h2>Accessing Services from Outside the Cluster (Ingress Setup)<a class="headerlink" href="#accessing-services-from-outside-the-cluster-ingress-setup" title="Link to this heading">#</a></h2>
<p id="id1">One common issue is making your services accessible from outside the cluster This is where Ingress comes into play, which defines the routing rules on external access to the services in a cluster. To set up Ingress, you need to have an Ingress controller installed in your cluster. I will be using the <a class="reference external" href="https://kubernetes.github.io/ingress-nginx">Nginx Ingress Controller</a> for this guide.</p>
<p>To set up Ingress, we’ll create a manifest file that defines both an IngressClass and an Ingress resource. An IngressClass specifies which controller should implement these rules, here we tell Kubernetes that we want to use the <em>NGINX Ingress Controller</em> for the class called <strong>nginx</strong>. In the Ingress specs, we specify that external requests to <em>ollama-ui.kube.local</em> will be directed to the “open-webui-service” on port 8080, which is the port open webui service is listening on.</p>
<div class="literal-block-wrapper docutils container" id="id27">
<div class="code-block-caption"><span class="caption-text">ingress-open-webui.yaml</span><a class="headerlink" href="#id27" title="Link to this code">#</a></div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nn">---</span>
<span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">networking.k8s.io/v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">IngressClass</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nginx</span>
<span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">controller</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">k8s.io/ingress-nginx</span>
<span class="nn">---</span>
<span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">networking.k8s.io/v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Ingress</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">open-webui-ingress</span>
<span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">open-webui</span>
<span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">ingressClassName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nginx</span>
<span class="w">  </span><span class="nt">rules</span><span class="p">:</span>
<span class="hll"><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">host</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ollama-ui.kube.local</span>
</span><span class="w">    </span><span class="nt">http</span><span class="p">:</span>
<span class="w">      </span><span class="nt">paths</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/</span>
<span class="w">        </span><span class="nt">pathType</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Prefix</span>
<span class="w">        </span><span class="nt">backend</span><span class="p">:</span>
<span class="w">          </span><span class="nt">service</span><span class="p">:</span>
<span class="hll"><span class="w">            </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">open-webui-service</span>
</span><span class="w">            </span><span class="nt">port</span><span class="p">:</span>
<span class="hll"><span class="w">              </span><span class="nt">number</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8080</span>
</span></pre></div>
</div>
</div>
<p>To expose the Ollama service, you can create a similar Ingress resource. You’ll need to modify the host, service name, and port number to match your Ollama service configuration.</p>
<div class="no-copybutton console-output highlight-console notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="gp">user@k8s-master-1:~$ </span>kubectl<span class="w"> </span>get<span class="w"> </span>ingress<span class="w"> </span>--all-namespaces
<span class="linenos">2</span><span class="go">NAMESPACE     NAME                 CLASS   HOSTS                  ADDRESS        PORTS   AGE</span>
<span class="linenos">3</span><span class="go">ns-ollama     ollama-ingress       nginx   ollama.kube.local      192.168.1.92   80      5d3h</span>
<span class="linenos">4</span><span class="go">open-webui    open-webui-ingress   nginx   ollama-ui.kube.local   192.168.1.92   80      4d</span>
<span class="linenos">5</span><span class="gp">user@k8s-master-1:~$ </span>kubectl<span class="w"> </span>get<span class="w"> </span>services<span class="w"> </span>-n<span class="w"> </span>ingress-nginx
<span class="linenos">6</span><span class="go">NAME                                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE</span>
<span class="linenos">7</span><span class="go">ingress-nginx-controller             NodePort    10.97.18.19    &lt;none&gt;        80:32528/TCP,443:31711/TCP   8d</span>
<span class="linenos">8</span><span class="go">ingress-nginx-controller-admission   ClusterIP   10.103.62.24   &lt;none&gt;        443/TCP                      8d</span>
</pre></div>
</div>
<p>The output shows that our Ingress resources are correctly set up for both the Ollama and Open WebUI services. The Ingress controller service is exposed using NodePort, which allows external traffic to reach the Ingress controller. Subsequently, it then routes the requests to the services based on the Ingress rules. You may consider using an external load balancer as well.</p>
<p>With this setup, you should now be able to access your Open WebUI service from outside the cluster using the hostname <a class="reference external" href="http://ollama-ui.kube.local:32528">http://ollama-ui.kube.local:32528</a>. Remember to ensure your DNS is also configured to resolve this hostname. I have set up a local DNS server using <a class="reference external" href="https://pi-hole.net/">Pi-hole</a>. Alternatively, you may add an entry <code class="code highlight bash docutils literal highlight-bash"><span class="o">{</span>node<span class="w"> </span>ip<span class="o">}</span><span class="w"> </span>ollama-ui.kube.local</code> to your hosts file in a client machine within the same network.</p>
<a class="reference internal image-reference" href="../../_images/open-webui-ollama-demo.gif"><img alt="Demo of Open WebUI for Ollama" class="align-center" src="../../_images/open-webui-ollama-demo.gif" style="width: 650px;" />
</a>
<p>It would be handy if you could access the LLM server from your mobile devices. <a class="reference external" href="https://github.com/AugustDev/enchanted">Enchanted</a> is a mobile client app that allows you to interact with Ollama from an IOS device. Ensure that you also expose Ollama service using Ingress. Simply connect the device to the same network where the LLM server is running, and in settings, enter the IP address with the of the server.</p>
<a class="reference internal image-reference" href="../../_images/enchanted-demo.jpg"><img alt="Demo of Enchanted for Ollama" class="align-center" src="../../_images/enchanted-demo.jpg" style="width: 650px;" />
</a>
<p>This should conclude the server side and the client side setup of running and using an LLM service in a home lab environment.</p>
</section>
</section>

<div class="section ablog__blog_comments">
  
  


<div class="section ablog__prev-next">
  <span class="ablog__prev">
    
    
    <a href="../set_up_kubernetes_cluster/">
      
      <i class="fa fa-arrow-circle-left"></i>
      
      <span>Deploying a Local Kubernetes Cluster on Ubuntu Servers</span>
    </a>
    
  </span>
  <span class="ablog__spacer">&nbsp;</span>
  <span class="ablog__next">
    
    
    <a href="../what_is_agentic_system/">
      <span>My Two Cents on Agentic AI</span>
      
      <i class="fa fa-arrow-circle-right" ></i>
      
    </a>
    
  </span>
</div>

  
  
</div>

                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up-a-gpu-supported-node-nvidia">Setting Up a GPU Supported Node (NVIDIA)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#installing-nvidia-container-toolkit">Installing NVIDIA Container Toolkit</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#installing-nvidia-gpu-operator-in-kubernetes">Installing NVIDIA GPU Operator in Kubernetes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#running-a-gpu-enabled-container-for-testing">Running a GPU Enabled Container (For Testing)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu-sharing-with-nvidia-gpu-operator">GPU Sharing with Nvidia GPU Operator</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up-ollama-on-kubernetes">Setting Up Ollama on Kubernetes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up-open-webui-for-ollama">Setting Up Open WebUI for Ollama</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#accessing-services-from-outside-the-cluster-ingress-setup">Accessing Services from Outside the Cluster (Ingress Setup)</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2024, Yi Q.
      <br/>
    
  </p>
</div>
      
    </div>
  
  
  
</div>

  </footer>
  </body>
</html>